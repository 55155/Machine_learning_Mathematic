{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_same_shape(array: ndarray,\n",
    "                      array_grad: ndarray):\n",
    "    assert array.shape == array_grad.shape, \\\n",
    "        '''\n",
    "        두 ndarray의 모양이 같아야 하는데,\n",
    "        첫 번째 ndarray의 모양은 {0}이고,\n",
    "        두 번째 ndarray의 모양은 {1}이다.\n",
    "        '''.format(tuple(array_grad.shape), tuple(array.shape))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Operation`, `ParamOperation` 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(object):\n",
    "    '''\n",
    "    신경망 모델의 연산 역할을 하는 기반 클래스\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_: ndarray): #forward 방향으로 진행하는 propagation\n",
    "        '''\n",
    "        인스턴스 변수 self._input에 입력값을 저장한 다음 self._output() 함수를 호출한다.\n",
    "        '''\n",
    "        self.input_ = input_ # input_ 은 ndarray의 형태로 들어온다.\n",
    "\n",
    "        self.output = self._output() # output은 _output 메소드에 따라 달라진다. \n",
    "        # 이때 _output 메소드는 상속받는 클래스에 존재하여야 한다.\n",
    "\n",
    "        return self.output # output을 return 한다.\n",
    "\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray: # backpropagation 과정\n",
    "        '''\n",
    "        self._input_grad() 함수를 호출한다. 이때 모양이 일치하는지 먼저 확인한다.\n",
    "        '''\n",
    "        assert_same_shape(self.output, output_grad) # outputsize와 outputgrad 는 당연히 같아야한다.\n",
    "\n",
    "        self.input_grad = self._input_grad(output_grad) # output_grad에 대한 input_grad를 정의한다.\n",
    "        # 연쇄법칙에 의해 계산되는 항\n",
    "\n",
    "        assert_same_shape(self.input_, self.input_grad) # input_grad 또한 마찬가지\n",
    "        return self.input_grad\n",
    "\n",
    "\n",
    "    def _output(self) -> ndarray: # output \n",
    "        '''\n",
    "        Operation을 구현한 모든 구상 클래스는 _output 메서드를 구현해야 한다.\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Operation을 구현한 모든 구상 클래스는 _input_grad 메서드를 구현해야 한다.\n",
    "        '''\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamOperation(Operation): # ParamOperation은 Operation을 상속한다.\n",
    "    '''\n",
    "    파라미터를 갖는 연산\n",
    "    '''\n",
    "\n",
    "    def __init__(self, param: ndarray) -> ndarray:\n",
    "        '''\n",
    "        생성자 메서드\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.param = param # parameter 즉, 가중치를 정의한다.\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray: # \n",
    "        '''\n",
    "        self._input_grad, self._param_grad를 호출한다.\n",
    "        이때 ndarray 객체의 모양이 일치하는지 확인한다.\n",
    "        '''\n",
    "\n",
    "        assert_same_shape(self.output, output_grad) # 상속자과 같은 과정\n",
    "\n",
    "        self.input_grad = self._input_grad(output_grad) \n",
    "        # input_grad 즉 출력값에 대한 입력값의 도함수를 구한다.\n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "        # param_grad 즉 출력값에 대한 parameter의 도함수를 구한다.\n",
    "\n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "        assert_same_shape(self.param, self.param_grad)\n",
    "\n",
    "        return self.input_grad\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        ParamOperation을 구현한 모든 구상 클래스는 _param_grad 메서드를 구현해야 한다.\n",
    "        '''\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Operation`의 구상 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightMultiply(ParamOperation): # 가중치 행렬곱 연산\n",
    "    '''\n",
    "    신경망의 가중치 행렬곱 연산\n",
    "    '''\n",
    "\n",
    "    def __init__(self, W: ndarray):\n",
    "        '''\n",
    "        self.param = W로 초기화\n",
    "        '''\n",
    "        super().__init__(W)\n",
    "\n",
    "    def _output(self) -> ndarray: # 단순히 X@W\n",
    "        '''\n",
    "        출력값 계산\n",
    "        '''\n",
    "        return np.dot(self.input_, self.param)\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray: # 단순히 \n",
    "        '''\n",
    "        입력에 대한 기울기 계산\n",
    "        '''\n",
    "        return np.dot(output_grad, np.transpose(self.param, (1, 0)))\n",
    "        # 연쇄법칙에 의한 연산\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray)  -> ndarray:\n",
    "        '''\n",
    "        파라미터에 대한 기울기 계산\n",
    "        '''        \n",
    "        return np.dot(np.transpose(self.input_, (1, 0)), output_grad)\n",
    "        # 마찬가지로 연쇄법칙에 의한 grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasAdd(ParamOperation):\n",
    "    '''\n",
    "    편향을 더하는 연산\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 B: ndarray):\n",
    "        '''\n",
    "        self.param = B로 초기화한다.\n",
    "        초기화 전에 행렬의 모양을 확인한다.\n",
    "        '''\n",
    "        assert B.shape[0] == 1 # B.shape[0] 은 1이어야 한다.\n",
    "        \n",
    "        super().__init__(B)\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        '''\n",
    "        출력값 계산\n",
    "        '''\n",
    "        return self.input_ + self.param\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        입력에 대한 기울기 계산\n",
    "        '''\n",
    "        return np.ones_like(self.input_) * output_grad\n",
    "        # Bias의 미분값 즉, B가 output에 얼마나 영향을 미치는지는 모두 상수 1임\n",
    "        # B 1단위 증가 시 output 또한 1단위 증가한다.\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        파라미터에 대한 기울기 계산\n",
    "        '''\n",
    "        param_grad = np.ones_like(self.param) * output_grad\n",
    "        # 마찬가지로 param_grad 는 \n",
    "        return np.sum(param_grad, axis=0).reshape(1, param_grad.shape[1])\n",
    "        # 이전 장에서 다뤘듯이 1차원으로 reshape 해주어야함. 이유는 B.shape이 1이기 떄문에 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Operation):\n",
    "    '''\n",
    "    Sigmoid 활성화 함수\n",
    "    '''\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        '''Pass'''\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        '''\n",
    "        출력값 계산\n",
    "        '''\n",
    "        return 1.0/(1.0+np.exp(-1.0 * self.input_))\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        입력에 대한 기울기 계산\n",
    "        '''\n",
    "        sigmoid_backward = self.output * (1.0 - self.output)\n",
    "        input_grad = sigmoid_backward * output_grad\n",
    "        return input_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Operation):\n",
    "    '''\n",
    "    항등 활성화 함수\n",
    "    '''\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        '''기반 클래스의 생성자 메서드 실행'''        \n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        '''입력을 그대로 출력'''\n",
    "        return self.input_\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''그대로 출력'''\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Layer`와 `Dense` 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    '''\n",
    "    신경망 모델의 층 역할을 하는 클래스\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 neurons: int):\n",
    "        '''\n",
    "        뉴런의 개수는 층의 너비에 해당한다\n",
    "        '''\n",
    "        self.neurons = neurons\n",
    "        self.first = True\n",
    "        self.params: List[ndarray] = []\n",
    "        self.param_grads: List[ndarray] = []\n",
    "        self.operations: List[Operation] = []\n",
    "\n",
    "    def _setup_layer(self, num_in: int) -> None:\n",
    "        '''\n",
    "        Layer를 구현하는 구상 클래스는 _setup_layer 메서드를 구현해야 한다\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, input_: ndarray) -> ndarray:\n",
    "        '''\n",
    "        입력값을 각 연산에 순서대로 통과시켜 순방향 계산을 수행한다.\n",
    "        ''' \n",
    "        if self.first:\n",
    "            self._setup_layer(input_)\n",
    "            self.first = False\n",
    "\n",
    "        self.input_ = input_\n",
    "\n",
    "        for operation in self.operations:\n",
    "            input_ = operation.forward(input_)\n",
    "\n",
    "        self.output = input_\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        output_grad를 각 연산에 역순으로 통과시켜 역방향 계산을 수행한다.\n",
    "        계산하기 전, 행렬의 모양을 검사한다.\n",
    "        '''\n",
    "\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        for operation in reversed(self.operations):\n",
    "            output_grad = operation.backward(output_grad)\n",
    "\n",
    "        input_grad = output_grad\n",
    "        \n",
    "        self._param_grads()\n",
    "\n",
    "        return input_grad\n",
    "\n",
    "    def _param_grads(self) -> ndarray:\n",
    "        '''\n",
    "        각 Operation 객체에서 _param_grad 값을 꺼낸다.\n",
    "        '''\n",
    "\n",
    "        self.param_grads = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.param_grads.append(operation.param_grad)\n",
    "\n",
    "    def _params(self) -> ndarray:\n",
    "        '''\n",
    "        각 Operation 객체에서 _params 값을 꺼낸다.\n",
    "        '''\n",
    "\n",
    "        self.params = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.params.append(operation.param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    '''\n",
    "    Layer 클래스를 구현한 전결합층\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 neurons: int,\n",
    "                 activation: Operation = Sigmoid()):\n",
    "        '''\n",
    "        초기화 시 활성화 함수를 결정해야 함\n",
    "        '''\n",
    "        super().__init__(neurons)\n",
    "        self.activation = activation\n",
    "\n",
    "    def _setup_layer(self, input_: ndarray) -> None:\n",
    "        '''\n",
    "        전결합층의 연산을 정의\n",
    "        '''\n",
    "        if self.seed:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        self.params = []\n",
    "\n",
    "        # 가중치\n",
    "        self.params.append(np.random.randn(input_.shape[1], self.neurons))\n",
    "\n",
    "        # 편향\n",
    "        self.params.append(np.random.randn(1, self.neurons))\n",
    "\n",
    "        self.operations = [WeightMultiply(self.params[0]),\n",
    "                           BiasAdd(self.params[1]),\n",
    "                           self.activation]\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Loss`와 `MeanSquaredError` 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    '''\n",
    "    신경망 모델의 손실을 계산하는 클래스\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        '''기반 클래스의 생성자 메서드를 실행'''\n",
    "        pass\n",
    "\n",
    "    def forward(self, prediction: ndarray, target: ndarray) -> float:\n",
    "        '''\n",
    "        실제 손실값을 계산함\n",
    "        '''\n",
    "        assert_same_shape(prediction, target)\n",
    "\n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "\n",
    "        loss_value = self._output()\n",
    "\n",
    "        return loss_value\n",
    "\n",
    "    def backward(self) -> ndarray:\n",
    "        '''\n",
    "        손실함수의 입력값에 대해 손실의 기울기를 계산함\n",
    "        '''\n",
    "        self.input_grad = self._input_grad()\n",
    "\n",
    "        assert_same_shape(self.prediction, self.input_grad)\n",
    "\n",
    "        return self.input_grad\n",
    "\n",
    "    def _output(self) -> float:\n",
    "        '''\n",
    "        Loss 클래스를 확장한 모든 구상 클래스는 이 메서드를 구현해야 함\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        '''\n",
    "        Loss 클래스를 확장한 모든 구상 클래스는 이 메서드를 구현해야 함\n",
    "        '''\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError(Loss):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        '''Pass'''\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> float:\n",
    "        '''\n",
    "        관찰 단위로 오차를 집계한 평균제곱오차 손실함수\n",
    "        '''\n",
    "        loss = (\n",
    "            np.sum(np.power(self.prediction - self.target, 2)) / \n",
    "            self.prediction.shape[0]\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        '''\n",
    "        예측값에 대한 평균제곱오차 손실의 기울기를 계산\n",
    "        '''        \n",
    "\n",
    "        return 2.0 * (self.prediction - self.target) / self.prediction.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `NeuralNetwork` 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    '''\n",
    "    신경망을 나타내는 클래스\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 layers: List[Layer],\n",
    "                 loss: Loss,\n",
    "                 seed: int = 1) -> None:\n",
    "        '''\n",
    "        신경망의 층과 손실함수를 정의\n",
    "        '''\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.seed = seed\n",
    "        if seed:\n",
    "            for layer in self.layers:\n",
    "                setattr(layer, \"seed\", self.seed)        \n",
    "\n",
    "    def forward(self, x_batch: ndarray) -> ndarray:\n",
    "        '''\n",
    "        데이터를 각 층에 순서대로 통과시킴(순방향 계산)\n",
    "        '''\n",
    "        x_out = x_batch\n",
    "        for layer in self.layers:\n",
    "            x_out = layer.forward(x_out)\n",
    "\n",
    "        return x_out\n",
    "\n",
    "    def backward(self, loss_grad: ndarray) -> None:\n",
    "        '''\n",
    "        데이터를 각 층에 역순으로 통과시킴(역방향 계산)\n",
    "        '''\n",
    "\n",
    "        grad = loss_grad\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def train_batch(self,\n",
    "                    x_batch: ndarray,\n",
    "                    y_batch: ndarray) -> float:\n",
    "        '''\n",
    "        순방향 계산 수행\n",
    "        손실값 계산\n",
    "        역방향 계산 수행\n",
    "        '''\n",
    "        \n",
    "        predictions = self.forward(x_batch) # prediction\n",
    "\n",
    "        loss = self.loss.forward(predictions, y_batch) # prediction - train_y\n",
    "\n",
    "        self.backward(self.loss.backward()) # self.backward(loss_grad)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def params(self):\n",
    "        '''\n",
    "        신경망의 파라미터 값을 받음\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            yield from layer.params\n",
    "\n",
    "    def param_grads(self):\n",
    "        '''\n",
    "        신경망의 각 파라미터에 대한 손실값의 기울기를 받음\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            yield from layer.param_grads    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Optimizer`와 `SGD` 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    '''\n",
    "    신경망을 최적화하는 기능을 제공하는 추상 클래스\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01):\n",
    "        '''\n",
    "        초기 학습률이 반드시 설정되어야 한다.\n",
    "        '''\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self) -> None:\n",
    "        '''\n",
    "        Optimizer를 구현하는 구상 클래스는 이 메서드를 구현해야 한다.\n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    '''\n",
    "    확률적 경사 하강법을 적용한 Optimizer\n",
    "    '''    \n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01) -> None:\n",
    "        '''Pass'''\n",
    "        super().__init__(lr)\n",
    "\n",
    "    def step(self):\n",
    "        '''\n",
    "        각 파라미터에 학습률을 곱해 기울기 방향으로 수정함\n",
    "        '''\n",
    "        for (param, param_grad) in zip(self.net.params(),\n",
    "                                       self.net.param_grads()):\n",
    "\n",
    "            param -= self.lr * param_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Trainer` 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 헬퍼 함수\n",
    "\n",
    "def permute_data(X, y):\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    return X[perm], y[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Tuple\n",
    "\n",
    "class Trainer(object):\n",
    "    '''\n",
    "    신경망 모델을 학습시키는 역할을 수행함\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 net: NeuralNetwork,\n",
    "                 optim: Optimizer) -> None:\n",
    "        '''\n",
    "        학습을 수행하려면 NeuralNetwork, Optimizer 객체가 필요함\n",
    "        Optimizer 객체의 인스턴스 변수로 NeuralNetwork 객체를 전달할 것\n",
    "        '''\n",
    "        self.net = net\n",
    "        self.optim = optim\n",
    "        self.best_loss = 1e9\n",
    "        setattr(self.optim, 'net', self.net)\n",
    "        \n",
    "    def generate_batches(self,\n",
    "                         X: ndarray,\n",
    "                         y: ndarray,\n",
    "                         size: int = 32) -> Tuple[ndarray]:\n",
    "        '''\n",
    "        배치 생성 \n",
    "        '''\n",
    "        assert X.shape[0] == y.shape[0], \\\n",
    "        '''\n",
    "        특징과 목푯값은 행의 수가 같아야 하는데,\n",
    "        특징은 {0}행, 목푯값은 {1}행이다\n",
    "        '''.format(X.shape[0], y.shape[0])\n",
    "\n",
    "        N = X.shape[0]\n",
    "\n",
    "        for ii in range(0, N, size):\n",
    "            X_batch, y_batch = X[ii:ii+size], y[ii:ii+size]\n",
    "\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "            \n",
    "    def fit(self, X_train: ndarray, y_train: ndarray,\n",
    "            X_test: ndarray, y_test: ndarray,\n",
    "            epochs: int=100,\n",
    "            eval_every: int=10,\n",
    "            batch_size: int=32,\n",
    "            seed: int = 1,\n",
    "            restart: bool = True)-> None:\n",
    "        '''\n",
    "        일정 횟수의 에폭을 수행하며 학습 데이터에 신경망을 최적화함\n",
    "        eval_every 변수에 설정된 횟수의 매 에폭마다 테스트 데이터로\n",
    "        신경망의 예측 성능을 측정함\n",
    "        '''\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        if restart:\n",
    "            for layer in self.net.layers:\n",
    "                layer.first = True\n",
    "\n",
    "            self.best_loss = 1e9\n",
    "\n",
    "        for e in range(epochs):\n",
    "\n",
    "            if (e+1) % eval_every == 0:\n",
    "                \n",
    "                # 조기 종료\n",
    "                last_model = deepcopy(self.net) \n",
    "                # 그때의 network를 저장\n",
    "\n",
    "            X_train, y_train = permute_data(X_train, y_train)\n",
    "\n",
    "            batch_generator = self.generate_batches(X_train, y_train,\n",
    "                                                    batch_size)\n",
    "\n",
    "            for ii, (X_batch, y_batch) in enumerate(batch_generator):\n",
    "\n",
    "                self.net.train_batch(X_batch, y_batch)\n",
    "\n",
    "                self.optim.step()\n",
    "\n",
    "            if (e+1) % eval_every == 0:\n",
    "\n",
    "                test_preds = self.net.forward(X_test)\n",
    "                loss = self.net.loss.forward(test_preds, y_test)\n",
    "\n",
    "                if loss < self.best_loss:\n",
    "                    print(f\"{e+1} 에폭에서 검증 데이터에 대한 손실값: {loss:.3f}\")\n",
    "                    self.best_loss = loss\n",
    "                else:\n",
    "                    print(f\"\"\"{e+1}에폭에서 손실값이 증가했다. 마지막으로 측정한 손실값은 {e+1-eval_every}에폭까지 학습된 모델에서 계산된 {self.best_loss:.3f}이다.\"\"\")\n",
    "                    self.net = last_model\n",
    "                    # self.optim이 self.net을 수정하도록 다시 설정\n",
    "                    setattr(self.optim, 'net', self.net)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평가 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_true: ndarray, y_pred: ndarray):\n",
    "    '''\n",
    "    신경망 모델의 평균절대오차 계산\n",
    "    '''    \n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def rmse(y_true: ndarray, y_pred: ndarray):\n",
    "    '''\n",
    "    신경망 모델의 제곱근 평균제곱오차 계산\n",
    "    '''\n",
    "    return np.sqrt(np.mean(np.power(y_true - y_pred, 2)))\n",
    "\n",
    "def eval_regression_model(model: NeuralNetwork,\n",
    "                          X_test: ndarray,\n",
    "                          y_test: ndarray):\n",
    "    '''\n",
    "    신경망 모델의 평균절대오차 및 제곱근 평균제곱오차 계산\n",
    "    Compute mae and rmse for a neural network.\n",
    "    '''\n",
    "    preds = model.forward(X_test)\n",
    "    preds = preds.reshape(-1, 1)\n",
    "    print(\"평균절대오차: {:.2f}\".format(mae(preds, y_test)))\n",
    "    print()\n",
    "    print(\"제곱근 평균제곱오차 {:.2f}\".format(rmse(preds, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = NeuralNetwork(\n",
    "    layers=[Dense(neurons=1,\n",
    "                   activation=Linear())],\n",
    "    loss=MeanSquaredError(),\n",
    "    seed=20190501\n",
    ")\n",
    "\n",
    "nn = NeuralNetwork(\n",
    "    layers=[Dense(neurons=13,\n",
    "                   activation=Sigmoid()),\n",
    "            Dense(neurons=1,\n",
    "                   activation=Linear())],\n",
    "    loss=MeanSquaredError(),\n",
    "    seed=20190501\n",
    ")\n",
    "\n",
    "dl = NeuralNetwork(\n",
    "    layers=[Dense(neurons=13,\n",
    "                   activation=Sigmoid()),\n",
    "            Dense(neurons=13,\n",
    "                   activation=Sigmoid()),\n",
    "            Dense(neurons=1,\n",
    "                   activation=Linear())],\n",
    "    loss=MeanSquaredError(),\n",
    "    seed=20190501\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 로드, 테스트 / 학습 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "data = diabetes.data\n",
    "target = diabetes.target\n",
    "features = diabetes.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 축척 변환\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "(442,)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_2d_np(a: ndarray, \n",
    "          type: str=\"col\") -> ndarray:\n",
    "    '''\n",
    "    1차원 텐서를 2차원으로 변환\n",
    "    '''\n",
    "\n",
    "    assert a.ndim == 1, \\\n",
    "    \"입력된 텐서는 1차원이어야 함\"\n",
    "    \n",
    "    if type == \"col\":        \n",
    "        return a.reshape(-1, 1)\n",
    "    elif type == \"row\":\n",
    "        return a.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=80718)\n",
    "\n",
    "# 목푯값을 2차원 배열로 변환\n",
    "y_train, y_test = to_2d_np(y_train), to_2d_np(y_test)\n",
    "# LOSS FUNCTION 을 구할 수 없는 문제가 생긴다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3가지 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 헬퍼 함수\n",
    "\n",
    "def permute_data(X, y):\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    return X[perm], y[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(preds: ndarray, actuals: ndarray):\n",
    "    '''\n",
    "    평균절대오차 계산\n",
    "    '''\n",
    "    return np.mean(np.abs(preds - actuals))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 에폭에서 검증 데이터에 대한 손실값: 3743.511\n",
      "20 에폭에서 검증 데이터에 대한 손실값: 2988.200\n",
      "30 에폭에서 검증 데이터에 대한 손실값: 2931.480\n",
      "40 에폭에서 검증 데이터에 대한 손실값: 2915.937\n",
      "50에폭에서 손실값이 증가했다. 마지막으로 측정한 손실값은 40에폭까지 학습된 모델에서 계산된 2915.937이다.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "154.51792064589398"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(lr, SGD(lr=0.01))\n",
    "\n",
    "trainer.fit(X_train, y_train, X_test, y_test,\n",
    "       epochs = 50,\n",
    "       eval_every = 10,\n",
    "       seed=20190501);\n",
    "print()\n",
    "mae(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 에폭에서 검증 데이터에 대한 손실값: 2916.445\n",
      "20 에폭에서 검증 데이터에 대한 손실값: 2845.746\n",
      "30 에폭에서 검증 데이터에 대한 손실값: 2815.692\n",
      "40에폭에서 손실값이 증가했다. 마지막으로 측정한 손실값은 30에폭까지 학습된 모델에서 계산된 2815.692이다.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "154.51792064589398"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(nn, SGD(lr=0.01))\n",
    "\n",
    "trainer.fit(X_train, y_train, X_test, y_test,\n",
    "       epochs = 50,\n",
    "       eval_every = 10,\n",
    "       seed=20190501);\n",
    "print()\n",
    "mae(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 에폭에서 검증 데이터에 대한 손실값: 44.143\n",
      "20 에폭에서 검증 데이터에 대한 손실값: 25.278\n",
      "30 에폭에서 검증 데이터에 대한 손실값: 22.339\n",
      "40 에폭에서 검증 데이터에 대한 손실값: 16.500\n",
      "50 에폭에서 검증 데이터에 대한 손실값: 14.655\n",
      "\n",
      "평균절대오차: 2.45\n",
      "\n",
      "제곱근 평균제곱오차 3.83\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(dl, SGD(lr=0.01))\n",
    "\n",
    "trainer.fit(X_train, y_train, X_test, y_test,\n",
    "       epochs = 50,\n",
    "       eval_every = 10,\n",
    "       seed=20190501);      \n",
    "print()\n",
    "eval_regression_model(dl, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
